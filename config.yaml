# LLM Cost-Efficiency Benchmarking Configuration

# LLM Models Configuration (2026 Pricing)
models:
  gpt-5-2:
    provider: openai
    model_name: gpt-5.2
    input_cost_per_1k: 0.015  # USD per 1K input tokens
    output_cost_per_1k: 0.060  # USD per 1K output tokens
    max_tokens: 128000
    
  gpt-5-pro:
    provider: openai
    model_name: gpt-5-pro
    input_cost_per_1k: 0.030
    output_cost_per_1k: 0.120
    max_tokens: 128000
    
  gemini-3-pro:
    provider: google
    model_name: gemini-3-pro
    input_cost_per_1k: 0.012
    output_cost_per_1k: 0.048
    max_tokens: 100000
    
  claude-4-5:
    provider: anthropic
    model_name: claude-4.5
    input_cost_per_1k: 0.018
    output_cost_per_1k: 0.072
    max_tokens: 200000
    
  llama-4-scout:
    provider: openai  # Using OpenAI-compatible endpoint
    model_name: llama-4-scout
    input_cost_per_1k: 0.005
    output_cost_per_1k: 0.020
    max_tokens: 32000

# Judge Model (for LLM-as-a-Judge)
judge_model:
  provider: openai
  model_name: gpt-5-pro
  
# Benchmark Configuration
benchmark:
  prompts_per_category: 50
  categories:
    - coding
    - summarization
    - creative_writing
  multi_turn_depth: 3
  
# Router Configuration
router:
  default_quality_threshold: 0.80
  min_historical_data: 5
  fallback_model: gpt-5-2
  
# Dashboard Configuration
dashboard:
  port: 8501
  refresh_interval: 60  # seconds
